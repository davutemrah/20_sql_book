--- 
title: "Statistics and Modelling"
author: "Davut Ayan"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: rstudio/bookdown-demo
description: "These are my statistics notes to my-self"
---

# Acknowledgement {-}


```{r setup, include=TRUE, eval=FALSE}
# Your setup code here
```

<!--chapter:end:index.Rmd-->

---
title: How Big Should the Control Group Be in a Randomized Field Experiment?
output: html_document
---


# control group size

```{r include=TRUE, eval=FALSE}
# Your setup code here
```

```{r echo = FALSE, message = FALSE, warning = FALSE, eval=FALSE}
library(tidyverse)
library(mgcv)
library(splines)
library(segmented)
dat <- read_csv("/Users/deayan/Desktop/GITHUB/5_Statistics/__REPO/01_AB_Test/Control_Group_Size/control_size_results.csv")

```

Research involves trade-offs. Basic social science—aimed at scientific discovery and theory-building—is dealing with a "replication crisis," and much of the debate between scholars stems from people valuing costs and benefits differently. **Some believe false positives are more dangerous to scientific advancement than are false negatives, while others feel the opposite.** This debate centers around trade-offs: If we have a stricter threshold for determining what is scientific evidence, we are going to make fewer wrong proclamations that an effect or relationship exists—but we are also going to miss out on interesting scientific discoveries. If we impose a looser threshold, the opposite is true.  

Social science in the real world involves additional, practical trade-offs that researchers and data scientists must manage. Such is the case when considering the current question of how large a control group should be in a randomized field experiment. For the purposes of this post, I consider an experimental design where participants are assigned to one of two conditions: a treatment or a control. I am defining the size of a control condition relative to the size of the sample: the proportion allocated to the control condition.  

Tensions in this situation involve the same as those in basic research—in addition to others. Randomized field experiments meet people where they are, in their day-to-day lives. This often requires considerable more resources than forcing a college sophomore to show up to your lab between classes. And the stakeholders invested in whatever it is we are testing—be it a technology, campaign, technique, or strategy—do not want to miss an opportunity we have to engage with people.  

This is especially true in politics. Consider a field experiment testing the effect an advertising or canvassing campaign has on voter turnout. **Every person we allocate to the control condition is a potential voter with whom we do not speak.** People who work hard to design advertisements want people to see their work; volunteers want to knock on doors and talk to people about the candidate they support. Elections only happen once, as well; we cannot go back later and contact the people who were in the control condition.  

These are just some of the trade-offs researchers and data scientists must consider in conducting field experiments. We want valid statistical inferences. We want to efficiently quantify the causal effect of these campaigns. It is clear to us that half of the participants should be in the treatment condition, while the other half is in the control condition, as this strategy gets us equally-precise estimates of the mean or frequency of the dependent variable in both conditions. But this means that we do not engage with a full 50% of the people we are targeting. So how many should we target? How can we manage these practical and methodological considerations? At the other extreme, we could expose 99.5% of the sample to the treatment and only 0.5% to the control. We are engaging the vast majority of people. And while we would have a very precise estimate of what is happening in the treatment condition, the standard error in the control condition would be so large that we would be clueless as to if it meaningfully differs from the treatment.  

The figure below illustrates this. I simulated data from an *N = 100* experiment where 50 people were in each condition ("50/50") or only 5 were in the control ("95/5"). Even though the effect is bigger in the latter situation, we cannot detect a significant effect—there are not enough people in the control condition (*p* = .102). However, there is a significant effect found with the 50/50 split (*p* = .015).  

```{r echo = TRUE, fig.align = "center", eval=FALSE}
set.seed(1839)
halfhalf <- tibble(
  x = rep(0:1, each = 50),
  y = rnorm(100, ifelse(x == 0, 0, .8))
)

t.test(y ~ x, halfhalf) # uncomment to see sig
```

```{r, eval=FALSE}
holdout5 <- tibble(
  x = c(rep(0, 5), rep(1, 95)),
  y = rnorm(100, ifelse(x == 0, 0, .8))
)
t.test(y ~ x, holdout5) # uncomment to see sig
```

```{r, eval=FALSE}
with(halfhalf, tapply(y, x, mean))
```

```{r echo = TRUE, fig.align = "center", eval=FALSE}

se <- function(x) sd(x) / sqrt(length(x))

illustrate <- tibble(
  split = c("50/50", "50/50", "95/5", "95/5"),
  cond = c("Control", "Treatment", "Control", "Treatment"),
  mean = c(with(halfhalf, tapply(y, x, mean)), 
           with(holdout5, tapply(y, x, mean))),
  se = c(with(halfhalf, tapply(y, x, se)),
         with(holdout5, tapply(y, x, se)))
)

ggplot(illustrate, aes(x = cond, y = mean)) +
  geom_point(size = 2) +
  geom_errorbar(aes(ymin = mean - 1.96 * se, 
                    ymax = mean + 1.96 * se),
                width = .2) +
  facet_grid(~ split) +
  theme_minimal() +
  theme(text = element_text(size = 20),
        legend.position = "none") +
  labs(x = "\nCondition", y = "Mean")
```

We could employ a mental calculus in determining the size of the control condition, weighing the opportunity costs associated with _not_ exposing someone to the treatment against the statistical efficiency of an even split between the conditions. This might serve us reasonably well, but my goal here is to quantitatively inform this calculus through a Monte Carlo simulation study, examining the relationship between statistical power and control group size. I now turn to discussing the methods of this study (i.e., how the simulation was done) before discussing results and implications.  

## Method

I decided each simulated dataset would mimic a randomized field experiment on voter turnout. Cases were assigned to either a "treatment" or a "control" condition. The outcome was either 1 ("Voted") or 0 ("Did Not Vote"). As these are simulated data, this could represent any other dichotomous outcome, such as 1 ("Favorable") or 0 ("Unfavorable"), etc.  

Second, I defined a range of characteristics for the data. These were:  

1. Sample size: *N* = 1000, 2500, 5000, 10000, 15000, or 20000.  

2. Control group size: 10%, 15%, 20%, 25%, and so on, up to 50% of the sample.  

3. Effect size: This was defined by lift, which is the percentage point increase in the positive outcome (e.g., voting, favorable opinion, donating, etc.) that the treatment had over the control. If 70% of people voted in the control condition and 72% did so in the treatment, this would represent a lift of 2 percentage points. Note that power depends on the rate of the positive outcome in the control condition. 

I set this constant across all datasets at 50%. This represents the best case scenario for power, given that power decreases as this control date approaches 0% or 100%. 

In preliminary simulations, however, I allowed this base rate to be 30%, 50%, or 70%. The this did not change the shape of the relationship between control size and power; it merely shifted the intercept up or down a little bit. Data from those preliminary simulations can be found [at GitHub](https://github.com/markhwhiteii/blog/tree/master/control_size).  

Third, I simulated 1,000 datasets for each of the 540 combinations of the characteristics above (i.e., 6 sample sizes times 9 control sizes times 10 effect sizes). For each dataset, cases were assigned to treatment or control conditions deterministically based on the control group size. If we let the size of the control group, *Ctl*, be the proportion of the sample in the control condition, and the size of the sample to be *N*, then *N x Ctl* cases were in the control, while *N x (1 - Ctl)* were in the treatment condition. For the control condition cases, the dependent variable for each case was drawn from a Binomial distribution, *B(1, 0.5)*, while the treatment case outcome variables were drawn from a distribution *B(1, 0.5 + L)*, where *L* denotes the lift.  

Fourth, *p*-values were obtained by regressing the outcome on the condition in a binomial logistic regression. For each of the 540 types of datasets, I calculated the proportion of the 1,000 simulations that yielded a *p*-value below .05. This represents the statistical power for that combination of data characteristics: It estimates the percentage of the time we would find a significant effect, given that it exists at that effect size. For example, if 800 of the 1,000 simulations yielded *p* < .05, then the power for that combination of data characteristics would be 80%.  

The resulting data I analyzed to examine the relationship between control size and power was one with 4 variables (N, control size, lift, and power) and 540 cases (each unique combinations of the N, control size, and lift possibilities).  

## Results

I started by generating curves illustrating the relationship between control size and power for each of the 60 *N x Lift* combinations, which are found below. All curves in the following analyses, unless where otherwise noted, were fitted using a natural cubic spline with 3 degrees of freedom (via the `mgcv` and `splines` R packages); all graphing was done using `ggplot2`.  

```{r echo = FALSE, fig.align = "center", out.width = "90%", eval=FALSE}
ggplot(dat, aes(x = ctl, y = power, color = factor(lift * 100))) +
  geom_point(size = 1) +
  geom_smooth(se = FALSE, method = "gam", alpha = .8,
              formula = y ~ ns(x, df = 3), size = 1) +
  facet_wrap(~ n, labeller = as_labeller(c(
    "1000" = "1,000",
    "2500" = "2,500",
    "5000" = "5,000",
    "10000" = "10,000",
    "15000" = "15,000",
    "20000" = "20,000"
  ))) +
  theme_minimal() +
  theme(axis.text.x = element_text(hjust = 1, angle = 45, size = 11),
        text = element_text(size = 16),
        legend.title = element_text(size = 14),
        legend.text = element_text(size = 14)) +
  labs(x = "Control Size", y = "Power") +
  scale_color_discrete(name = "Lift") +
  scale_x_continuous(label = function(x) paste0(x * 100, "%")) +
  scale_y_continuous(label = function(x) paste0(x * 100, "%"))
```

The first lesson from these curves is that deviating from a 50/50 treatment and control split harms statistical power. This is true in just about every circumstance, except when analyses are so tremendously over or underpowered that it does not matter.  

Below the 20% control size mark, even analyses that have about 100% power in the 50/50 case can start to lose power. Consider the 4 point lift curve when *N* = 20,000 (bottom right). Even though the study has about 100% power when holding out only 20% of the sample as control cases, this starts to tail off below 20% (granted, most researchers would still be happy with the level of power at the 10% control size).  

Most of these curves, however, do not represent typical situations data scientists face. The lower bound of power generally seen as "acceptable" is 80%, so we try to achieve at least that. We seldom find ourselves in situations when we have greater than 95% power, either. To get a more realistic picture, I now turn to only looking at the curves that achieve between 80% and 95% power when the control size is half of the sample. The horizontal dotted lines represent these 80% and 95% thresholds.  

```{r echo = FALSE, fig.align = "center", out.width = "65%", eval=FALSE}
dat %>% 
  group_by(n, lift) %>% 
  filter(max(power) > .8 & max(power) < .95) %>% 
  ggplot(aes(x = ctl, y = power, color = paste0(lift, n))) +
  geom_point() +
  geom_smooth(se = FALSE, method = "gam", formula = y ~ ns(x, df = 3)) +
  geom_hline(aes(yintercept = .8), linetype = 2) +
  geom_hline(aes(yintercept = .95), linetype = 2) +
  theme_minimal() +
  theme(axis.text.x = element_text(hjust = 1, angle = 45),
        text = element_text(size = 20),
        legend.position = "none") +
  labs(x = "Control Size", y = "Power") +
  scale_x_continuous(label = function(x) paste0(x * 100, "%")) +
  scale_y_continuous(label = function(x) paste0(x * 100, "%")) +
  coord_cartesian(ylim = c(.35, 1))
```

If we were already teetering on having 80% power in the 50/50 split situation, we lose that once we dip below about a 40% control size. Since these curves otherwise follow the same form, I decided to just fit one curve to all of these data points, collapsing across the *N x Lift* combinations.  

```{r echo = FALSE, fig.align = "center", out.width = "65%", eval=FALSE}
dat %>% 
  group_by(n, lift) %>% 
  filter(max(power) > .8 & max(power) < .95) %>% 
  ggplot(aes(x = ctl, y = power)) +
  geom_point(color = "#474A2C") +
  geom_smooth(se = FALSE, method = "gam", formula = y ~ ns(x, df = 3),
              color = "#564D80") +
  geom_hline(aes(yintercept = .8), linetype = 2, color = "grey10") +
  geom_hline(aes(yintercept = .95), linetype = 2, color = "grey10") +
  theme_minimal() +
  theme(axis.text.x = element_text(hjust = 1, angle = 45),
        text = element_text(size = 20),
        legend.position = "none") +
  labs(x = "Control Size", y = "Power") +
  scale_x_continuous(label = function(x) paste0(x * 100, "%")) +
  scale_y_continuous(label = function(x) paste0(x * 100, "%")) +
  coord_cartesian(ylim = c(.35, 1))
```

Things start to dip once we go below the 30% control size mark, and the 80% power threshold is crossed just below that point. Just by looking at the curve, I guessed that power really starts to drop off around 25% to 30% control size. To be a bit more precise than eyeballing, I fit a piecewise linear regression to these data (via the `segmented` R package). This involves fitting one straight line on cases above a certain threshold and another straight line on cases below it. Doing so allows us to get an idea of a single breakpoint where the relationship between control size and power changes.  

But where do we set the breakpoint? Estimation requires the analyst to take a guess at where it might be, and an iterative procedure searches around this until it converges on a solution that fit the data well. I specified my initial guess at a control size of 30%. The breakpoint was estimated at 23.2%, with a 95% confidence interval from 19.5% to 26.9%. I think of this as the "elbow" of the curve, where loss of power accelerates. In reality, there is not truly one turning point—the loss of power follows a smooth curve. But estimating this breakpoint gives us a useful approximation. It helps us have a mental benchmark of an area we should not go below when determining the size of control groups.  

```{r echo = FALSE, fig.align = "center", out.width = "65%", eval=FALSE}
fit_lm_dat <- dat %>% 
  group_by(n, lift) %>% 
  filter(max(power) > .8 & max(power) < .95)
fit_lm <- lm(power ~ ctl, fit_lm_dat)
fit_sgmt <- segmented(fit_lm, ~ctl, .3)
psi_ci <- confint(fit_sgmt)$ctl
ggplot(fit_lm_dat, aes(x = ctl, y = power)) +
  geom_point(color = "#474A2C") +
  geom_smooth(method = "lm", se = FALSE, 
              formula = y ~ x + I((x - psi_ci[1]) * I(psi_ci[1] > x)),
              color = "#564D80") +
  geom_segment(aes(x = psi_ci[2], xend = psi_ci[3], 
                   y = .4, yend = .4), color = "#E03616") +
  coord_cartesian(ylim = c(.35, 1)) +
  geom_point(mapping = aes(x = psi_ci[1], y = .4), color = "#5C0029",
             shape = "diamond", size = 3) +
  theme_minimal() +
  theme(axis.text.x = element_text(hjust = 1, angle = 45),
        text = element_text(size = 20),
        legend.position = "none") +
  labs(x = "Control Size", y = "Power") +
  scale_x_continuous(label = function(x) paste0(x * 100, "%")) +
  scale_y_continuous(label = function(x) paste0(x * 100, "%")) +
  geom_hline(aes(yintercept = .8), linetype = 2, color = "grey10")
```

## Takeaways

The best scenario, statistically-speaking, is an even split between treatment and control.  

We must often consider many factors outside of the research design and statistical world; research involves trade-offs. The decision of how many people to allocate to a control group should be based on a collaboration between relevant parties involved in the research process. In addition to this collective judgment on the opportunity costs of not exposing participants to the treatment, some important takeaways from this simulation study to remember are:  

1. Minimal losses in power occur when we shrink the control size to 40%.  

2. A 25% to 30% range is a good compromise, as this exposes 70% of the sample to the treatment, yet still does not harm power terribly.  

3. You should *not* allocate less than 20% of the sample to the control condition, save for situations when you are looking for large effects (e.g., 8 point lifts) and/or using large samples (e.g., 15,000 participants).  

I did not perform an exhaustive simulation of all possible scenarios, of course. If you would like to examine a case specific to your interests, explore these data, or replicate the results, all of the code can be found [at GitHub](https://github.com/markhwhiteii/blog/tree/master/control_size).  

<!--chapter:end:__REPO/01_AB_Test/Control_Group_Size/Control_Group_Size.Rmd-->

---
title: "power analysis"
author: "dea"
date: "`r Sys.Date()`"
output: html_document
---


## Power Analysis

### Overview

Power analysis is an important aspect of experimental design. It allows us to determine the sample size required to detect an effect of a given size with a given degree of confidence. Conversely, it allows us to determine the probability of detecting an effect of a given size with a given level of confidence, under sample size constraints. If the probability is unacceptably low, we would be wise to alter or abandon the experiment.

The following four quantities have an intimate relationship:

a. **sample size**

b. **effect size**

c. **significance level** = P(Type I error) = probability of finding an effect that is not there

d. **power** = 1 - P(Type II error) = probability of finding an effect that is there


Given any three, we can determine the fourth.



### Power Analysis in R
The `pwr` package develped by Stéphane Champely, impliments power analysis as outlined by Cohen (1988). Some of the more important functions are listed below.

function	power calculations for
`pwr.2p.test`	two proportions (equal n)
`pwr.2p2n.test`	two proportions (unequal n)
`pwr.anova.test`	balanced one way ANOVA
`pwr.chisq.test`	chi-square test
`pwr.f2.test`	general linear model
`pwr.p.test`	proportion (one sample)
`pwr.r.test`	correlation
`pwr.t.test`	t-tests (one sample, 2 sample, paired)
`pwr.t2n.test`	t-test (two samples with unequal n)

For each of these functions, you enter three of the four quantities (effect size, sample size, significance level, power) and the fourth is calculated.

The significance level defaults to 0.05. Therefore, to calculate the significance level, given an effect size, sample size, and power, use the option "sig.level=NULL".

Specifying an effect size can be a daunting task. ES formulas and Cohen's suggestions (based on social science research) are provided below. Cohen's suggestions should only be seen as very rough guidelines. Your own subject matter experience should be brought to bear.

(To explore confidence intervals and drawing conclusions from samples try this interactive course on the foundations of inference.)

#### t-tests

For t-tests, use the following functions:


pwr.t2n.test(n1 = , n2= , d = , sig.level =, power = ) where n1 and n2 are the sample sizes.

For t-tests, the effect size is assessed as

d = |mu1 - mu2|/ var

Cohen suggests that d values of 0.2, 0.5, and 0.8 represent small, medium, and large effect sizes respectively.

You can specify alternative="two.sided", "less", or "greater" to indicate a two-tailed, or one-tailed test. A two tailed test is the default.


The `pwr.t.test` function in R is used to perform power analysis for t-tests. It helps you determine the sample size needed to achieve a certain level of power for a specified effect size and significance level. Here's a simple example using `pwr.t.test`:

Let's say you want to perform a two-sample t-test to compare the means of two groups. You want to know how many samples per group you need to achieve 80% power to detect a difference of 2 units between the group means, assuming a standard deviation of 5 and a significance level of 0.05.

```{R}
# Load the pwr package
library(pwr)

# pwr.t.test(n = NULL, d = NULL, 
#    sig.level = 0.05, power = NULL, 
#    type = c("two.sample", "one.sample", "paired"),
#    alternative = c("two.sided", "less", "greater"))`
    
# where n is the sample size, d is the effect size, and type indicates a two-sample t-test, # # one-sample t-test or paired t-test. If you have unequal sample sizes, use


# Set parameters for the power analysis

## effect_size = abs(mean1 - mean2)/sd_pooled

effect_size <- .2 # Difference in means
sd_pooled <- 5    # Pooled standard deviation
alpha <- 0.05      # Significance level
power <- 0.80      # Desired power

# Perform power analysis
result <- pwr.t.test(d = effect_size, 
                     #sd = sd_pooled, 
                     sig.level = alpha, 
                     power = power, 
                     type = "two.sample")

# Display the result
print(result)
```

In this example:

- `d` is the effect size (difference in means divided by the standard deviation).
- `sd` is the pooled standard deviation.
- `sig.level` is the significance level (usually set to 0.05).
- `power` is the desired power level.

The `type` argument specifies the type of t-test, and in this case, it's a "two.sample" t-test.

The `pwr.t.test` function will return a list with information about the power analysis, including the sample size needed per group.

```{r}
# pwr.t2n.test(n1 = NULL, n2= NULL, d = NULL, sig.level = 0.05, power = NULL,
#  alternative = c("two.sided", 
#         "less","greater"))

pwr.t2n.test(n1=100, d=0.5, sig.level = 0.05, power = 0.8)
```

```{r}
# pwr.t2n.test(n1 = NULL, n2= NULL, d = NULL, sig.level = 0.05, power = NULL,
#  alternative = c("two.sided", 
#         "less","greater"))

pwr.t2n.test(n1=100, n2=10, d=.5, sig.level = 0.05)
```

1. Required control size increases as effect size (d) decreases

2. Required control size increases as significance level decreases


### Some Examples

```{r}
library(pwr)

# For a one-way ANOVA comparing 5 groups, calculate the
# sample size needed in each group to obtain a power of
# 0.80, when the effect size is moderate (0.25) and a
# significance level of 0.05 is employed.

pwr.anova.test(k=5,f=.25,sig.level=.05,power=.8)

# What is the power of a one-tailed t-test, with a
# significance level of 0.01, 25 people in each group,
# and an effect size equal to 0.75?

pwr.t.test(n=25,d=0.75,sig.level=.01,alternative="greater")

# Using a two-tailed test proportions, and assuming a
# significance level of 0.01 and a common sample size of
# 30 for each proportion, what effect size can be detected
# with a power of .75?

pwr.2p.test(n=30,sig.level=0.01,power=0.75)
```

```{r}
library(pwr)

# range of correlations
r <- seq(.1,.5,.01)
nr <- length(r)

# power values
p <- seq(.4,.9,.1)
np <- length(p)

# obtain sample sizes
samsize <- array(numeric(nr*np), dim=c(nr,np))
for (i in 1:np){
  for (j in 1:nr){
    result <- pwr.r.test(n = NULL, r = r[j],
    sig.level = .05, power = p[i],
    alternative = "two.sided")
    samsize[j,i] <- ceiling(result$n)
  }
}

# set up graph
xrange <- range(r)
yrange <- round(range(samsize))
colors <- rainbow(length(p))
plot(xrange, yrange, type="n",
  xlab="Correlation Coefficient (r)",
  ylab="Sample Size (n)" )

# add power curves
for (i in 1:np){
  lines(r, samsize[,i], type="l", lwd=2, col=colors[i])
}

# add annotation (grid lines, title, legend)
abline(v=0, h=seq(0,yrange[2],50), lty=2, col="grey89")
abline(h=0, v=seq(xrange[1],xrange[2],.02), lty=2,
   col="grey89")
title("Sample Size Estimation for Correlation Studies\n
  Sig=0.05 (Two-tailed)")
legend("topright", title="Power",
as.character(p),
   fill=colors)
```




<!--chapter:end:__REPO/01_AB_Test/Control_Group_Size/power_analysis/power_analysis.Rmd-->


### Statistical power

Statistical power is a crucial concept in experimental design and hypothesis testing. It represents the probability that a statistical test will correctly reject a false null hypothesis, or in other words, the ability of a study to detect a true effect if it exists. 

A study with low statistical power is more likely to produce false-negative results, which means it might not detect a real effect even when it is present.

Here are key points related to statistical power in experimental design:

1. **Components of Statistical Power:**
   - **Effect Size:** The magnitude of the difference or the strength of the relationship you are trying to detect.
   - **Sample Size:** The number of participants or observations in your study.
   - **Significance Level (α):** The probability of rejecting a true null hypothesis (usually set at 0.05).
   - **Power (1-β):** The probability of correctly rejecting a false null hypothesis.

2. **Factors Influencing Statistical Power:**

   - **Effect Size:** Larger effects are easier to detect and result in higher power.
   - **Sample Size:** Increasing the sample size generally increases power.
   - **Significance Level:** Lowering the significance level increases power, but it also increases the likelihood of Type II errors (false negatives).

3. **Trade-offs between Type I and Type II Errors:**
   - **Type I Error (α):** The probability of incorrectly rejecting a true null hypothesis.
   - **Type II Error (β):** The probability of failing to reject a false null hypothesis.
   - There is often a trade-off between Type I and Type II errors. Decreasing the probability of one type of error typically increases the probability of the other.

4. **Power Analysis:**
   - Power analysis is a crucial step in experimental design. It helps researchers determine the required sample size to achieve a desired level of power for detecting a specific effect size.
   - Researchers typically perform power analyses before conducting experiments to ensure that the study has a reasonable chance of detecting meaningful effects.

5. **Interpretation of Power:**
   - High power (close to 1) indicates a greater ability to detect real effects.
   - Low power (close to 0) suggests a higher likelihood of missing real effects.
   - Conventionally, a power of **0.80 (or 80%)** is often considered acceptable in many fields.

In summary, statistical power is a critical consideration in experimental design, and researchers need to carefully plan studies to achieve adequate power. This involves choosing an appropriate sample size based on the expected effect size, setting significance levels, and performing power analyses to ensure the study is capable of detecting meaningful effects.




<!--chapter:end:__REPO/01_AB_Test/Control_Group_Size/power_analysis/power.Rmd-->

## Multicollinearity

Multicollinearity occurs when two or more independent variables in a regression model are highly correlated, meaning they have a strong linear relationship. This high correlation can lead to several problems in statistical modeling, particularly in multiple linear regression, where the goal is to isolate the effect of each independent variable on the dependent variable.

### Key Points About Multicollinearity:

1. **Inflated Variances of Coefficients:**
   Multicollinearity makes it difficult to determine the individual effect of each independent variable. This happens because the model cannot distinguish between the effects of correlated variables, leading to large standard errors for the estimated coefficients. As a result, even if the variables are theoretically important, their coefficients might not appear statistically significant.

2. **Unstable Coefficient Estimates:**
   The presence of multicollinearity can cause the coefficients to be highly sensitive to changes in the model. Small changes in the data or the model specification can lead to large swings in coefficient estimates, making the model less reliable.

3. **Reduced Interpretability:**
   When variables are highly correlated, it's challenging to interpret the effect of one variable while holding others constant, because in reality, those variables are not independent of each other.

### How to Detect Multicollinearity:

- **Variance Inflation Factor (VIF):**
  VIF measures how much the variance of a coefficient is inflated due to multicollinearity. A VIF value greater than 10 is often considered an indicator of significant multicollinearity, though the threshold can vary depending on the context.

- **Correlation Matrix:**
  A simple way to detect multicollinearity is by examining the correlation matrix of the independent variables. High correlations (e.g., above 0.8 or 0.9) between two variables suggest potential multicollinearity.

### Handling Multicollinearity:

1. **Remove or Combine Variables:**
   If two variables are highly correlated, you might consider removing one of them or combining them into a single variable (e.g., by taking their average or sum).

2. **Principal Component Analysis (PCA):**
   PCA can be used to reduce the dimensionality of the data by creating uncorrelated components from the original variables, thereby addressing multicollinearity.

3. **Regularization Techniques:**
   Methods like Ridge regression or Lasso regression add a penalty to the regression coefficients, which can help mitigate the effects of multicollinearity by shrinking the coefficients of correlated variables.

Understanding and addressing multicollinearity is crucial in building robust and interpretable models, especially in complex datasets where multiple factors might be interrelated.


<!--chapter:end:__REPO/1_.Rmd-->


## Transformations

### Log transformation

• To correct for right skewness
• When considering ratios
• In settings where errors are feasibly multiplicative, such as when dealing with concentrations or rates
• To consider orders of magnitude (using log base 10); for example, when considering astronomical distances
• Counts are often logged (though note the problem with zero counts). Income variable is generally log-transformed.

```{r, echo=FALSE, eval=FALSE}
knitr::include_graphics("files/log_transform.png") 
```


<!--chapter:end:__REPO/1_1_log_transform.Rmd-->


## Permutation testing


Permutation testing, also known as randomization or re-randomization testing, is a `non-parametric` statistical method used to assess the significance of an observed outcome by comparing it to the distribution of possible outcomes under a null hypothesis. The primary idea behind permutation testing is to generate a large number of permutations of the observed data, compute a test statistic for each permutation, and then compare the observed test statistic to the distribution of permuted test statistics to determine if the observed result is statistically significant.

Here is a general outline of the permutation testing process:

1. **Define the Null Hypothesis (H0):** The null hypothesis typically states that there is no effect, no difference, or no association in the population.

2. **Select a Test Statistic:** Choose a test statistic that measures the effect or difference you are interested in. This could be a mean difference, correlation coefficient, regression coefficient, etc.

3. **Collect and Shuffle the Data:** Combine the data from different groups or conditions and shuffle the data to create random permutations. The number of permutations depends on the computational resources available and the desired precision.

4. **Compute the Test Statistic for Each Permutation:** For each permutation, calculate the test statistic based on the shuffled data.

5. **Compare Observed Statistic to Permuted Distribution:** Compare the observed test statistic to the distribution of permuted test statistics. The p-value is the proportion of permuted test statistics that are more extreme than the observed test statistic.

6. **Draw Conclusions:** If the p-value is below a predetermined significance level (e.g., 0.05), you may reject the null hypothesis in favor of the alternative hypothesis.

Permutation testing is particularly useful in situations where assumptions of parametric tests may not be met or when dealing with complex study designs. It provides a distribution-free approach to hypothesis testing, making it more robust in certain situations. However, permutation testing can be computationally intensive, especially for large datasets, as it requires generating and testing a large number of permutations.

<!--chapter:end:__REPO/1_2_permutation.Rmd-->

